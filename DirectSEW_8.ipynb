{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EH69gRef9Og",
        "outputId": "ca557abe-ed98-4147-a03a-5e29f7a7adaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spikingjelly -q"
      ],
      "metadata": {
        "id": "eUMpdUVrgYwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import errno\n",
        "import math\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.distributed as dist\n",
        "from torch import amp\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "from spikingjelly.clock_driven.neuron import MultiStepParametricLIFNode\n",
        "from spikingjelly.clock_driven import layer, functional\n"
      ],
      "metadata": {
        "id": "QnRGfrvshvQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from spikingjelly.clock_driven.neuron import MultiStepParametricLIFNode\n",
        "# from spikingjelly.clock_driven import layer\n",
        "\n",
        "def conv3x3(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        layer.SeqToANNContainer(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        ),\n",
        "        MultiStepParametricLIFNode(init_tau=2.0, detach_reset=True)\n",
        "    )\n",
        "\n",
        "def conv1x1(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        layer.SeqToANNContainer(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        ),\n",
        "        MultiStepParametricLIFNode(init_tau=2.0, detach_reset=True)\n",
        "    )\n",
        "\n",
        "class SEWBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, connect_f=None):\n",
        "        super(SEWBlock, self).__init__()\n",
        "        self.connect_f = connect_f\n",
        "        self.conv = nn.Sequential(\n",
        "            conv3x3(in_channels, mid_channels),\n",
        "            conv3x3(mid_channels, in_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        out = self.conv(x)\n",
        "        if self.connect_f == 'ADD':\n",
        "            out += x\n",
        "        elif self.connect_f == 'AND':\n",
        "            out *= x\n",
        "        elif self.connect_f == 'IAND':\n",
        "            out = x * (1. - out)\n",
        "        else:\n",
        "            raise NotImplementedError(self.connect_f)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PlainBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels):\n",
        "        super(PlainBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            conv3x3(in_channels, mid_channels),\n",
        "            conv3x3(mid_channels, in_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.conv(x)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            conv3x3(in_channels, mid_channels),\n",
        "\n",
        "            layer.SeqToANNContainer(\n",
        "                nn.Conv2d(mid_channels, in_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "            ),\n",
        "        )\n",
        "        self.sn = MultiStepParametricLIFNode(init_tau=2.0, detach_reset=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.sn(x + self.conv(x))\n",
        "\n",
        "\n",
        "class ResNetN(nn.Module):\n",
        "    def __init__(self, layer_list, num_classes, connect_f=None):\n",
        "        super(ResNetN, self).__init__()\n",
        "        in_channels = 2\n",
        "        conv = []\n",
        "\n",
        "        for cfg_dict in layer_list:\n",
        "            channels = cfg_dict['channels']\n",
        "\n",
        "            if 'mid_channels' in cfg_dict:\n",
        "                mid_channels = cfg_dict['mid_channels']\n",
        "            else:\n",
        "                mid_channels = channels\n",
        "\n",
        "            if in_channels != channels:\n",
        "                if cfg_dict['up_kernel_size'] == 3:\n",
        "                    conv.append(conv3x3(in_channels, channels))\n",
        "                elif cfg_dict['up_kernel_size'] == 1:\n",
        "                    conv.append(conv1x1(in_channels, channels))\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "            in_channels = channels\n",
        "\n",
        "\n",
        "            if 'num_blocks' in cfg_dict:\n",
        "                num_blocks = cfg_dict['num_blocks']\n",
        "                if cfg_dict['block_type'] == 'sew':\n",
        "                    for _ in range(num_blocks):\n",
        "                        conv.append(SEWBlock(in_channels, mid_channels, connect_f))\n",
        "                elif cfg_dict['block_type'] == 'plain':\n",
        "                    for _ in range(num_blocks):\n",
        "                        conv.append(PlainBlock(in_channels, mid_channels))\n",
        "                elif cfg_dict['block_type'] == 'basic':\n",
        "                    for _ in range(num_blocks):\n",
        "                        conv.append(BasicBlock(in_channels, mid_channels))\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "            if 'k_pool' in cfg_dict:\n",
        "                k_pool = cfg_dict['k_pool']\n",
        "                conv.append(layer.SeqToANNContainer(nn.MaxPool2d(k_pool, k_pool)))\n",
        "\n",
        "        conv.append(nn.Flatten(2))\n",
        "\n",
        "        self.conv = nn.Sequential(*conv)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros([1, 1, 128, 128])\n",
        "            for m in self.conv.modules():\n",
        "                if isinstance(m, nn.MaxPool2d):\n",
        "                    x = m(x)\n",
        "            out_features = x.numel() * in_channels\n",
        "\n",
        "        self.out = nn.Linear(out_features, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.permute(1, 0, 2, 3, 4)  # [T, N, 2, *, *]\n",
        "        x = self.conv(x)\n",
        "        return self.out(x.mean(0))\n",
        "\n",
        "def SEWResNet(connect_f):\n",
        "    layer_list = [\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'sew', 'k_pool': 2},\n",
        "    ]\n",
        "    num_classes = 5\n",
        "    return ResNetN(layer_list, num_classes, connect_f)\n",
        "\n",
        "def PlainNet(*args, **kwargs):\n",
        "    layer_list = [\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'plain', 'k_pool': 2},\n",
        "    ]\n",
        "    num_classes = 11\n",
        "    return ResNetN(layer_list, num_classes)\n",
        "\n",
        "def SpikingResNet(*args, **kwargs):\n",
        "    layer_list = [\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "        {'channels': 32, 'up_kernel_size': 1, 'mid_channels': 32, 'num_blocks': 1, 'block_type': 'basic', 'k_pool': 2},\n",
        "    ]\n",
        "    num_classes = 11\n",
        "    return ResNetN(layer_list, num_classes)"
      ],
      "metadata": {
        "id": "1LCT28-tgtSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict, deque\n",
        "# import datetime\n",
        "# import time\n",
        "# import torch\n",
        "# import torch.distributed as dist\n",
        "\n",
        "# import errno\n",
        "# import os\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {}'.format(header, total_time_str))\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target[None])\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].flatten().sum(dtype=torch.float32)\n",
        "            res.append(correct_k * (100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:\n",
        "            raise\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    elif hasattr(args, \"rank\"):\n",
        "        pass\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pPVNjkWJg8io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    class Args:\n",
        "        model = \"SEWResNet\"\n",
        "        train_data_path = \"/content/drive/MyDrive/npz_events_best\"\n",
        "        test_data_path = \"/content/drive/MyDrive/npz_events_test_best\"\n",
        "        device = \"cuda\"\n",
        "        batch_size = 16\n",
        "        epochs = 25\n",
        "        workers = 4\n",
        "        lr = 5e-4\n",
        "        momentum = 0.9\n",
        "        weight_decay = 1e-4\n",
        "        lr_step_size = 64\n",
        "        lr_gamma = 0.1\n",
        "        print_freq = 64\n",
        "        output_dir = \"/content/drive/MyDrive/logs\"\n",
        "        resume = \"\"\n",
        "        start_epoch = 0\n",
        "        sync_bn = False\n",
        "        test_only = False\n",
        "        amp = True\n",
        "        world_size = 1\n",
        "        dist_url = \"env://\"\n",
        "        tb = True\n",
        "        adam = True\n",
        "        connect_f = \"ADD\"\n",
        "        T_train = 12\n",
        "\n",
        "    return Args()\n"
      ],
      "metadata": {
        "id": "08e3EkW736-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_seed_ = 2020\n",
        "import random\n",
        "random.seed(2020)\n",
        "\n",
        "torch.manual_seed(_seed_)  # use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA)\n",
        "torch.cuda.manual_seed_all(_seed_)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(_seed_)\n",
        "\n",
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class SpikingjellyDataset:\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.files = []  # Store (file_path, label) tuples\n",
        "\n",
        "        # Iterate through each folder (gesture class)\n",
        "        for class_label, class_folder in enumerate(sorted(os.listdir(dataset_path))):\n",
        "            class_path = os.path.join(dataset_path, class_folder)\n",
        "            if os.path.isdir(class_path):  # Ensure it's a directory\n",
        "                for file in sorted(os.listdir(class_path)):\n",
        "                    if file.endswith(\".npz\"):  # Only use .npz files\n",
        "                        self.files.append((os.path.join(class_path, file), class_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path, label = self.files[idx]  # Get file path and label\n",
        "        data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "        # Ensure keys exist\n",
        "        required_keys = {\"x\", \"y\", \"t\", \"p\", \"f\"}\n",
        "        if not required_keys.issubset(data.files):\n",
        "            raise ValueError(f\"Missing keys in {file_path}: {set(data.files) - required_keys}\")\n",
        "\n",
        "        x = data[\"x\"].astype(np.float32)\n",
        "        y = data[\"y\"].astype(np.float32)\n",
        "        t = data[\"t\"].astype(np.float32)\n",
        "        p = data[\"p\"].astype(np.float32)\n",
        "        folder_name = data[\"f\"].item()\n",
        "\n",
        "        events = np.stack([x, y, t, p], axis=1)  # Shape: (num_events, 4)\n",
        "\n",
        "        return torch.from_numpy(events), label, folder_name\n",
        "\n",
        "# Loader Class for Batch Processing\n",
        "class Loader:\n",
        "    def __init__(self, dataset, args, device, distributed, batch_size, drop_last=True , to_train = True):\n",
        "        self.device = device\n",
        "        if distributed is True:\n",
        "            self.sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
        "        else:\n",
        "            if to_train:\n",
        "                self.sampler = torch.utils.data.RandomSampler(dataset)\n",
        "            else:self.sampler = torch.utils.data.SequentialSampler(dataset)\n",
        "\n",
        "        self.loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=self.sampler,\n",
        "                                                  num_workers=args.workers, pin_memory=True,\n",
        "                                                  collate_fn=collate_events, drop_last=drop_last)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for data in self.loader:\n",
        "            data = [d.to(self.device) for d in data]\n",
        "            yield data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loader)\n",
        "\n",
        "# Collate function to handle batching of events\n",
        "def collate_events(data):\n",
        "    labels = []\n",
        "    events = []\n",
        "    folder_names = []\n",
        "    for i, d in enumerate(data):\n",
        "        labels.append(d[1])\n",
        "        folder_names.append(d[2])\n",
        "        ev = torch.cat([d[0], i * torch.ones((len(d[0]), 1), dtype=torch.float32)], 1)\n",
        "        events.append(ev)\n",
        "    events = torch.cat(events, 0)\n",
        "    labels = default_collate(labels)\n",
        "    folder_names = default_collate(folder_names)\n",
        "    return events, labels, folder_names\n",
        "\n",
        "class QuantizationLayerVoxGrid(nn.Module):\n",
        "    def __init__(self, dim, mode):\n",
        "        nn.Module.__init__(self)\n",
        "        self.dim = dim\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, events):\n",
        "        epsilon = 10e-3\n",
        "        B = int(1+events[-1, -1].item())\n",
        "        # tqdm.write(str(B))\n",
        "        num_voxels = int(2 * np.prod(self.dim) * B)\n",
        "        C, H, W = self.dim\n",
        "        vox = events[0].new_full([num_voxels, ], fill_value=0)\n",
        "        # get values for each channel\n",
        "        x, y, t, p, b = events.T\n",
        "        x = x.to(torch.int64)\n",
        "        y = y.to(torch.int64)\n",
        "        p = p.to(torch.int64)\n",
        "        b = b.to(torch.int64)\n",
        "        # normalizing timestamps\n",
        "        unit_len = []\n",
        "        t_idx = []\n",
        "        for bi in range(B):\n",
        "            bi_idx = events[:, -1] == bi\n",
        "            t[bi_idx] /= t[bi_idx].max()\n",
        "            unit_len.append(int(bi_idx.float().sum() / C))\n",
        "            _, t_idx_ = torch.sort(t[events[:, -1] == bi])\n",
        "            t_idx.append(t_idx_)\n",
        "        idx_before_bins = x \\\n",
        "                          + W * y \\\n",
        "                          + 0 \\\n",
        "                          + W * H * C * p \\\n",
        "                          + W * H * C * 2 * b\n",
        "\n",
        "\n",
        "        for i_bin in range(C):\n",
        "            values = torch.zeros_like(t)\n",
        "            for bi in range(B):\n",
        "                bin_idx = t_idx[bi][i_bin * unit_len[bi]: (i_bin + 1) * unit_len[bi]]\n",
        "                bin_values = values[events[:, -1] == bi]\n",
        "                bin_values[bin_idx] = 1\n",
        "                values[events[:, -1] == bi] = bin_values\n",
        "            # draw in voxel grid\n",
        "            idx = idx_before_bins + W * H * i_bin\n",
        "            vox.put_(idx.long(), values, accumulate=True)\n",
        "\n",
        "        vox = vox.view(-1, 2, C, H, W)#.clamp(0, 1)\n",
        "        if self.mode == \"TB\":\n",
        "            vox = vox.permute(2, 0, 1, 3, 4).contiguous()\n",
        "        elif self.mode == \"BT\":\n",
        "            vox = vox.permute(0, 2, 1, 3, 4).contiguous()\n",
        "        else:\n",
        "            raise Exception\n",
        "        return vox\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, quantizer,print_freq, scaler=None, T_train=None):\n",
        "    model.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))\n",
        "    metric_logger.add_meter('img/s', SmoothedValue(window_size=10, fmt='{value}'))\n",
        "\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    for event, target , foldername in data_loader:\n",
        "        start_time = time.time()\n",
        "        image = quantizer(event)\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        image = image.float()  # [N, T, C, H, W]\n",
        "\n",
        "        if T_train:\n",
        "            sec_list = np.random.choice(image.shape[1], T_train, replace=False)\n",
        "            sec_list.sort()\n",
        "            image = image[:, sec_list]\n",
        "\n",
        "        if scaler is not None:\n",
        "            with torch.amp.autocast(device_type=\"cuda\"):\n",
        "                output = model(image)\n",
        "                loss = criterion(output, target)\n",
        "        else:\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        functional.reset_net(model)\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        batch_size = image.shape[0]\n",
        "        loss_s = loss.item()\n",
        "        if math.isnan(loss_s):\n",
        "            raise ValueError('loss is Nan')\n",
        "        acc1_s = acc1.item()\n",
        "        acc5_s = acc5.item()\n",
        "\n",
        "        metric_logger.update(loss=loss_s, lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        metric_logger.meters['acc1'].update(acc1_s, n=batch_size)\n",
        "        metric_logger.meters['acc5'].update(acc5_s, n=batch_size)\n",
        "        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    return metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_loader, device,quantizer, print_freq=100, header='Test:'):\n",
        "    model.eval()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    with torch.no_grad():\n",
        "        for event, target , foldername in data_loader:\n",
        "            image = quantizer(event)\n",
        "            image = image.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "            image = image.float()\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            functional.reset_net(model)\n",
        "\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            batch_size = image.shape[0]\n",
        "            metric_logger.update(loss=loss.item())\n",
        "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
        "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "\n",
        "    loss, acc1, acc5 = metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg\n",
        "    # print(f' * Acc@1 = {acc1}, Acc@5 = {acc5}, loss = {loss}')\n",
        "    return loss, acc1, acc5\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    max_test_acc1 = 0.\n",
        "    test_acc5_at_max_test_acc1 = 0.\n",
        "\n",
        "\n",
        "    train_tb_writer = None\n",
        "    te_tb_writer = None\n",
        "\n",
        "    init_distributed_mode(args)\n",
        "    print(args)\n",
        "\n",
        "    output_dir = os.path.join(args.output_dir, f'{args.model}_b{args.batch_size}')\n",
        "\n",
        "    if args.T_train:\n",
        "        output_dir += f'_Ttrain{args.T_train}'\n",
        "\n",
        "    if args.weight_decay:\n",
        "        output_dir += f'_wd{args.weight_decay}'\n",
        "\n",
        "    output_dir += f'_steplr{args.lr_step_size}_{args.lr_gamma}'\n",
        "\n",
        "    if args.adam:\n",
        "        output_dir += '_adam'\n",
        "    else:\n",
        "        output_dir += '_sgd'\n",
        "\n",
        "    if args.connect_f:\n",
        "        output_dir += f'_cnf_{args.connect_f}'\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        mkdir(output_dir)\n",
        "\n",
        "    output_dir = os.path.join(output_dir, f'lr{args.lr}')\n",
        "    if not os.path.exists(output_dir):\n",
        "        mkdir(output_dir)\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # data_path = args.data_path\n",
        "    dataset_train = SpikingjellyDataset(args.train_data_path)\n",
        "    dataset_test = SpikingjellyDataset(args.test_data_path)\n",
        "\n",
        "    print(f\"dataset_train {len(dataset_train)} , dataset_test {len(dataset_test)}\")\n",
        "\n",
        "    distributed = False\n",
        "    batch_size  = 16\n",
        "    data_loader = Loader(dataset=dataset_train, args=args, device=device, distributed=distributed, batch_size=batch_size)\n",
        "    data_loader_test = Loader(dataset=dataset_test, args=args, device=device, distributed=distributed, batch_size=batch_size , to_train = False)\n",
        "\n",
        "    quantizer = QuantizationLayerVoxGrid(dim=(16, 128 ,128), mode=\"BT\")\n",
        "\n",
        "    model = SEWResNet(args.connect_f)\n",
        "    print(\"Creating model\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    num_params = count_trainable_params(model)\n",
        "    print(f\"Total Trainable Parameters: {num_params:,}\")\n",
        "    if args.distributed and args.sync_bn:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if args.adam:\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.SGD(\n",
        "            model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "    if args.amp:\n",
        "        scaler = torch.amp.GradScaler() if args.amp else None\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    # if args.resume:\n",
        "    #     checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "    #     model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "    #     optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    #     lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "    #     args.start_epoch = checkpoint['epoch'] + 1\n",
        "    #     max_test_acc1 = checkpoint['max_test_acc1']\n",
        "    #     test_acc5_at_max_test_acc1 = checkpoint['test_acc5_at_max_test_acc1']\n",
        "\n",
        "\n",
        "    # if args.tb and is_main_process():\n",
        "    #     purge_step_train = args.start_epoch\n",
        "    #     purge_step_te = args.start_epoch\n",
        "    #     train_tb_writer = SummaryWriter(output_dir + '_logs/train', purge_step=purge_step_train)\n",
        "    #     te_tb_writer = SummaryWriter(output_dir + '_logs/te', purge_step=purge_step_te)\n",
        "    #     with open(output_dir + '_logs/args.txt', 'w', encoding='utf-8') as args_txt:\n",
        "    #         args_txt.write(str(args))\n",
        "\n",
        "    #     print(f'purge_step_train={purge_step_train}, purge_step_te={purge_step_te}')\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        save_max = False\n",
        "        if args.distributed:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "        train_loss, train_acc1, train_acc5 = train_one_epoch(model, criterion, optimizer, data_loader, device, epoch,quantizer, args.print_freq, scaler, args.T_train )\n",
        "        # print(f\"Train Loss {train_loss} , Train_acc1 = {train_acc1} , Train acc5 = {train_acc5}\")\n",
        "        # if is_main_process():\n",
        "        #     train_tb_writer.add_scalar('train_loss', train_loss, epoch)\n",
        "        #     train_tb_writer.add_scalar('train_acc1', train_acc1, epoch)\n",
        "        #     train_tb_writer.add_scalar('train_acc5', train_acc5, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        test_loss, test_acc1, test_acc5 = evaluate(model, criterion, data_loader_test,quantizer=quantizer, device=device, header='Test:')\n",
        "        print(f\"Epoch {epoch} ,Train Loss {train_loss} , Train_acc1 = {train_acc1} , Train acc5 = {train_acc5} , test_loss = {test_loss} , test_acc_1 ={test_acc1}\")\n",
        "        # if te_tb_writer is not None:\n",
        "        #     if is_main_process():\n",
        "\n",
        "        #         te_tb_writer.add_scalar('test_loss', test_loss, epoch)\n",
        "        #         te_tb_writer.add_scalar('test_acc1', test_acc1, epoch)\n",
        "        #         te_tb_writer.add_scalar('test_acc5', test_acc5, epoch)\n",
        "\n",
        "\n",
        "        # if max_test_acc1 < test_acc1:\n",
        "        #     max_test_acc1 = test_acc1\n",
        "        #     test_acc5_at_max_test_acc1 = test_acc5\n",
        "        #     save_max = True\n",
        "\n",
        "\n",
        "        # if output_dir:\n",
        "\n",
        "        #     checkpoint = {\n",
        "        #         'model': model_without_ddp.state_dict(),\n",
        "        #         'optimizer': optimizer.state_dict(),\n",
        "        #         'lr_scheduler': lr_scheduler.state_dict(),\n",
        "        #         'epoch': epoch,\n",
        "        #         'args': args,\n",
        "        #         'max_test_acc1': max_test_acc1,\n",
        "        #         'test_acc5_at_max_test_acc1': test_acc5_at_max_test_acc1,\n",
        "        #     }\n",
        "\n",
        "            # if save_max:\n",
        "            #     save_on_master(\n",
        "            #         checkpoint,\n",
        "            #         os.path.join(output_dir, 'checkpoint_max_test_acc1.pth'))\n",
        "        # print(args)\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "\n",
        "        # print('Training time {}'.format(total_time_str), 'max_test_acc1', max_test_acc1, 'test_acc5_at_max_test_acc1', test_acc5_at_max_test_acc1)\n",
        "        # print(output_dir)\n",
        "    # if output_dir:\n",
        "    #     save_on_master(\n",
        "    #         checkpoint,\n",
        "    #         os.path.join(output_dir, f'checkpoint_{epoch}.pth'))\n",
        "\n",
        "    return max_test_acc1\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnnxgvB1hCtj",
        "outputId": "197c591f-4b9d-404e-e6ac-b8ec0efa48ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "<__main__.parse_args.<locals>.Args object at 0x796e091edc10>\n",
            "dataset_train 8600 , dataset_test 1252\n",
            "Creating model\n",
            "Total Trainable Parameters: 130,228\n",
            "Start training\n",
            "Epoch 0 ,Train Loss 1.1465527811529916 , Train_acc1 = 55.24906890130354 , Train acc5 = 100.0 , test_loss = 1.115483219233843 , test_acc_1 =55.12820512820513\n",
            "Epoch 1 ,Train Loss 0.8607932464592506 , Train_acc1 = 68.45903165735568 , Train acc5 = 100.0 , test_loss = 1.177893294690129 , test_acc_1 =54.006410256410255\n",
            "Epoch 2 ,Train Loss 0.7235828130826826 , Train_acc1 = 74.39478584729981 , Train acc5 = 100.0 , test_loss = 0.6894715288892771 , test_acc_1 =75.64102564102564\n",
            "Epoch 3 ,Train Loss 0.6296454407760329 , Train_acc1 = 78.0842644320298 , Train acc5 = 100.0 , test_loss = 0.6348970941721629 , test_acc_1 =77.00320512820512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgtL0obQjLXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}